<!DOCTYPE HTML>
<html>
    <head>
        <title>Implicit Behavioral Cloning</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=1000">
        <link rel="stylesheet" href="https://use.typekit.net/quv7bsd.css"> <!-- fonts -->
        <link rel="stylesheet" href="style.css" />
    </head>
    <body id="body">
        <div id="main"> 
            <header id="header">
            </header>
            <!-- style="padding-bottom:1em" -->
            <div id="profile">
                <!-- <img src="images/profile.jpg"> -->
                <div id="profile-desc">
                    <div id="profile-name"><b>Implicit Behavioral Cloning</b>
                        <p><br>Conference on Robot Learning (CoRL) 2021</p><br>
                        <p><a href="https://github.com/google-research/ibc">Code (GitHub)</a> &nbsp &nbsp <a href="https://arxiv.org/abs/2109.00137">Paper (arXiv)</a><p>
                    </div>
                    <p>
                        <b>Abstract</b>. We find that across a wide range of robot policy learning scenarios, treating supervised policy learning with an implicit model generally performs better, on average, than commonly used explicit models. We present extensive experiments on this finding, and we provide both intuitive insight and theoretical arguments distinguishing the properties of implicit models compared to their explicit counterparts, particularly with respect to approximating complex, potentially discontinuous and multi-valued (set-valued) functions. On robotic policy learning tasks we show that implicit behavioral cloning policies with energy-based models (EBM) often outperform common explicit (Mean Square Error, or Mixture Density) behavioral cloning policies, including on tasks with high-dimensional action spaces and visual image inputs. We find these policies provide competitive results or outperform state-of-the-art offline reinforcement learning methods on the challenging human-expert tasks from the D4RL benchmark suite, despite using no reward information. In the real world, robots with implicit policies can learn complex and remarkably subtle behaviors on contact-rich tasks from human demonstrations, including tasks with high combinatorial complexity and tasks requiring 1mm precision.
                    </p>
                </div>
                <div style="clear: both;"></div>
            </div>

            <div class="section paper">
                <h1 style="margin-bottom: 0px">Highlights</h1>
                <div class="section recent-work">
                    <div class="highlight-proj">
                        <video height="240" width="100%" playsinline="" muted="" autoplay="" loop="">
                            <source src="images/ebm_step_fit_crop.mp4" type="video/mp4">
                        </video>
                        <p style="font-size: 16px; margin-top: 0px; margin-left: 0px">Implicit models &#10084;&#65039; discontinuities.</p>
                    </div>
                    <div class="highlight-proj">
                        <video height="240" width="100%" playsinline="" muted="" autoplay="" loop="">
                            <source src="images/insertion-small.mp4" type="video/mp4">
                        </video>
                        <p style="font-size: 16px; margin-top: 0px; margin-left: 0px">Real-world slide-then-insert task <br> with 1-mm precision.</p>
                    </div>
                    <div class="highlight-proj">
                        <video height="240" width="100%" playsinline="" muted="" autoplay="" loop="">
                            <source src="images/sort-robust-small.mp4" type="video/mp4">
                        </video>
                        <p style="font-size: 16px; margin-top: 0px; margin-left: 0px">Real-world sorting task <br> with combinatorial multimodality.</p>
                    </div>
                </div>
                <div class="section recent-work">
                    <div class="highlight-proj">
                        <video height="240" width="100%" playsinline="" muted="" autoplay="" loop="">
                            <source src="images/particle_langevin_H264.mp4" type="video/mp4">
                        </video>
                        <p style="font-size: 16px; margin-left: 0px">Precise cloning of <br> switching behaviors. <br> (shown: "2D Particle" task)</p>
                    </div>
                    <div class="highlight-proj">
                        <video height="240" width="100%" playsinline="" muted="" autoplay="" loop="">
                            <source src="images/ebm_door_crop.mp4" type="video/mp4">
                        </video>
                        <p style="font-size: 16px; margin-left: 0px">State-of-the art results on D4RL human-expert tasks. <br> (shown: "door-human-v0" task)</p>
                    </div>
                    <div class="highlight-proj">
                        <video height="240" width="100%" playsinline="" muted="" autoplay="" loop="">
                            <source src="images/bimanual-sweeping-ebm-best-crop.mp4" type="video/mp4">
                        </video>
                        <p style="font-size: 16px; margin-left: 0px">Works with high-dimensional observations and inputs. <br> (shown: "Bi-Manual Sweeping")</p>
                    </div>
                </div>
            </div>
                <div class="divider"></div>
                <br>

                                     <div class="section paper">
                <h1>Paper</h1>
                <p>Latest version (Sep 1, 2021): <a href="https://arxiv.org/abs/2109.00137">arXiv:2109.00137 [cs.RO]</a>.<br>Published at the Conference on Robot Learning (CoRL) 2021<br>
                <p style="text-align:center"><br> 
                <a href="https://arxiv.org/pdf/2109.00137.pdf"><img style="width:500px" src="images/tiled_paper_small.jpg"></a>
                </p>
            </div>
            <div class="divider"></div>


            <div class="section paper">
                <h1>Policy Learning Results</h1>
                <p><b>Real World Tasks:</b> We find Implicit BC policies can work especially well in the real world, including 10x better on a precise insertion task (with only input as an RGB image at 5 Hz).</p><br>

                
                <video height="540" width="100%" playsinline="" muted="" autoplay="" loop="">
                    <source src="images/insertion-sideview-1top.mp4" type="video/mp4">
                </video>
                <p style="font-size: 16px; text-align:center">Implicit BC policy on a precise, 1-millimeter-tolerance <br>slide-then-insert task: push a block across a table, then slide it into a slot.</p><br>

                <p>Below is a representative comparison on this insertion task:</p><br>
            </div>
            
            <div class="section recent-work">

                <div class="highlight-proj2by">
                    <video height="460" width="100%" playsinline="" muted="" autoplay="" loop="">
                        <source src="images/ebm_insert.mp4" type="video/mp4">
                    </video>
                    <p style="font-size: 16px">Implicit BC (ours)</p>
                </div>
                <div class="highlight-proj2by">
                    <video height="460" width="100%" playsinline="" muted="" autoplay="" loop="">
                        <source src="images/mse_insert.mp4" type="video/mp4">
                    </video>
                    <p style="font-size: 16px">Explicit BC (baseline)</p>
                </div>
            </div>

            <div class="section paper">
                <p> And many examples of different IBC rollouts -- note the diversity of trajectories:</p><br>

                <video height="480" width="100%" playsinline="" muted="" autoplay="" loop="">
                    <source src="images/ibc_insert_tiled.mp4" type="video/mp4">
                </video>
                <p style="font-size: 16px; text-align:center">Implicit BC (ours)</p><br>

                <p>These policies can be robust in the real world, despite disturbances. (this is on the "Sorting" task: sort the 4 blue blocks from the 4 yellow blocks, in arbitrary order).</p><br>

                <video height="480" width="100%" playsinline="" muted="" autoplay="" loop="">
                    <source src="images/sort-robust.mp4" type="video/mp4">
                </video>
                <p style="font-size: 16px; text-align:center">Implicit BC policy on a sort-blue-from-yellow task with a high amount of multimodality <br>due to the combinatorially large number of valid orderings.</p><br>

                <p>On this sorting task, here is a representative comparison:</p><br>
            </div>


            <div class="section recent-work">

                <div class="highlight-proj2by">
                    <video height="360" width="100%" playsinline="" muted="" autoplay="" loop="">
                        <source src="images/ebm_sort_crop.mp4" type="video/mp4">
                    </video>
                    <p style="font-size: 16px">Implicit BC (ours)</p>
                </div>
                <div class="highlight-proj2by">
                    <video height="360" width="100%" playsinline="" muted="" autoplay="" loop="">
                        <source src="images/mse_sort_crop.mp4" type="video/mp4">
                    </video>
                    <p style="font-size: 16px">Explicit BC (baseline)</p>
                </div>
                
            </div>

            <div class="section paper">
            <p>Below are several additional comparisons on various simulated tasks.  See the paper for more analysis and interpretation
                on the different aspects highlighted by these comparisons.</p><br>
            </div>


            <div class="divider"></div>
            <br>



 
            <div class="section paper">
                <p><b>"Particle" Task</b>: agent (black dot) should move to the green dot, then the blue dot.</p><br>
            </div>           
            <div class="section recent-work">

                <div class="highlight-proj2by">
                    <video height="240" width="100%" playsinline="" muted="" autoplay="" loop="">
                        <source src="images/particle_langevin_H264.mp4" type="video/mp4">
                    </video>
                    <p style="font-size: 16px">Implicit BC (ours)</p>
                </div>
                <div class="highlight-proj2by">
                    <video height="240" width="100%" playsinline="" muted="" autoplay="" loop="">
                        <source src="images/particle_mse_H264.mp4" type="video/mp4">
                    </video>
                    <p style="font-size: 16px">Explicit BC (baseline)</p>
                </div>
            </div>
            <div class="divider"></div>
            <br>


            <div class="section paper">
            <p><b>"Planar Sweeping" Task</b>: agent (stick) should sweep all particles into the target region.</p><br>
            </div>
            <div class="section recent-work">
                

                <div class="highlight-proj2by">
                    <video height="240" width="100%" playsinline="" muted="" autoplay="" loop="">
                        <source src="images/pymunk-sweeping-ebm-best.mp4" type="video/mp4">
                    </video>
                    <p style="font-size: 16px">Implicit BC (ours)</p>
                </div>
                <div class="highlight-proj2by">
                    <video height="240" width="100%" playsinline="" muted="" autoplay="" loop="">
                        <source src="images/pymunk-sweeping-mse-best.mp4" type="video/mp4">
                    </video>
                    <p style="font-size: 16px">Explicit BC (baseline)</p>
                </div>
            </div>
            <div class="divider"></div>
            <br>

            <div class="section paper">
            <p><b>"Bi-Manual Sweeping" Task</b>: agent (two robot arms) should sweep all particles into the two bowls.</p><br>
            </div>
            <div class="section recent-work">
                <div class="highlight-proj2by">
                    <video height="240" width="100%" playsinline="" muted="" autoplay="" loop="">
                        <source src="images/bimanual-sweeping-ebm-best-crop.mp4" type="video/mp4">
                    </video>
                    <p style="font-size: 16px">Implicit BC (ours)</p>
                </div>
                <div class="highlight-proj2by">
                    <video height="240" width="100%" playsinline="" muted="" autoplay="" loop="">
                        <source src="images/bimanual-sweeping-mse-best-crop.mp4" type="video/mp4">
                    </video><!-- </a> -->
                    <p style="font-size: 16px">Explicit BC (baseline)</p>
                </div>
            </div>
            <br>
            <div class="divider"></div>

            <div class="section paper">
            <br>
            <p><b>"D4RL (Adroit) Door" Task</b>: agent (robot hand) should open the door.</p><br>
            </div>
            <div class="section recent-work">
                <div class="highlight-proj2by">
                    <video height="240" width="100%" playsinline="" muted="" autoplay="" loop="">
                        <source src="images/ebm_door_crop.mp4" type="video/mp4">
                    </video>
                    <p style="font-size: 16px">Implicit BC (ours)</p>
                </div>
                <div class="highlight-proj2by">
                    <video height="240" width="100%" playsinline="" muted="" autoplay="" loop="">
                        <source src="images/mse_door_crop.mp4" type="video/mp4">
                    </video><!-- </a> -->
                    <p style="font-size: 16px">Explicit BC (baseline)</p>
                </div>
            </div>
            <br>
            <div class="divider"></div>

            <div class="section paper">
            <br>
            <p><b>"D4RL (Adroit) Pen" Task</b>: agent (robot hand) should orient the pen.</p><br>
            </div>
            <div class="section recent-work">
                <br>
                

                <div class="highlight-proj2by">
                    <video height="240" width="100%" playsinline="" muted="" autoplay="" loop="">
                        <source src="images/ebm_pen_crop.mp4" type="video/mp4">
                    </video>
                    <p style="font-size: 16px">Implicit BC (ours)</p>
                </div>
                <div class="highlight-proj2by">
                    <video height="240" width="100%" playsinline="" muted="" autoplay="" loop="">
                        <source src="images/mse_pen_crop.mp4" type="video/mp4">
                    </video><!-- </a> -->
                    <p style="font-size: 16px">Explicit BC (baseline)</p>
                </div>
            </div>
            <br>
            <div class="divider"></div>

            <div class="section paper">
                <h1>Generalization, Discontinuities, and Multimodality</h1>
                <p>We highlight in particular three properties of implicit models vs. their explicit counterparts.<p><br>

                <p>For <b>generalization</b>, implicit models empirically show intriguingly different generalization properties compared to explicit counterparts, which in certain cases can be advantageous, for example in this coordinate regression task:

                <p style="text-align:center"><br> 
                <img style="width:600px" src="images/coord_reg.png">
                </p>
                <p style="font-size: 16px; text-align:center">In a canonical coordinate regression task, implicit models generalize well from just a few examples (shown above is 10 training examples), whereas explicit models struggle. (See Paper for more details).</p><br>

                <p>For <b>discontinuities</b>, the following animation illustrates training an implicit (top) vs. explicit (bottom) model
                on a simple <a href="https://en.wikipedia.org/wiki/Heaviside_step_function">Heaviside step function</a>.</p>

                
                <video height="360" width="100%" playsinline="" muted="" autoplay="" loop="">
                    <source src="images/ebm_step_fit.mp4" type="video/mp4">
                </video>
                <p style="font-size: 16px; text-align:center">(above) Implicit MLP trained as an EBM.</p>

                
                <video height="360" width="100%" playsinline="" muted="" autoplay="" loop="">
                    <source src="images/mse_step_fit.mp4" type="video/mp4">
                </video>
                <p style="font-size: 16px; text-align:center">(above) Explicit MLP trained with Mean Square Error.</p><br>

                <p>For approximating <b>multi-valued functions and/or conditional multi-modal distributions</b>, implicit models benefit from unconstrained flexibility.</p><br>

                <p style="text-align:center"><br> 
                <img style="width:800px" src="images/multi-valued.png">
                </p>
                <p style="font-size: 16px; text-align:center">Comparison of implicit (top) fitting of multi-valued functions compared to an example explicit model (bottom), using mixture-of-gaussian regression ("Mixture Density Network").</p><br>




            </div>
            <br>
            <br>
            <div class="divider"></div>


            <div class="section paper half">
                <h1>Code</h1>
                <p>
                    <a href="https://github.com/google-research/ibc">Code is available on Github!</a> Includes:<br>
                    <p style="font-size: 16px">
                    &nbsp;&nbsp;&bull;&nbsp;&nbsp;Simulation environments: Particle, Simulated Robot, D4RL tasks.<br>
                    &nbsp;&nbsp;&bull;&nbsp;&nbsp;Downloadable data for tasks.<br>
                    &nbsp;&nbsp;&bull;&nbsp;&nbsp;IBC implementation (Langevin and DFO).<br>
                    &nbsp;&nbsp;&bull;&nbsp;&nbsp;MSE and MDN baselines.<br>
                    &nbsp;&nbsp;&bull;&nbsp;&nbsp;Configurations for trainings.<br>
                    &nbsp;&nbsp;&bull;&nbsp;&nbsp;10-minute Quickstart.<br>
                    <!-- &nbsp;&nbsp;&bull;&nbsp;&nbsp;Pre-trained models and datasets.<br><br> -->
                    <!-- Simple toy examples with JAX and Flax in <a href="https://colab.research.google.com/drive/13VTA8nLM8AuzXuygQA1X_KWzLLxVPJhM?usp=sharing">Colab</a>.<br> -->
                </p>
                </p>

            </div>
            <div class="section bibtex half">
                <h1>Bibtex</h1>
                <div class="code">@article{florence2021implicit,<br>
&nbsp;&nbsp;&nbsp;&nbsp;title={Implicit Behavioral Cloning},<br>
&nbsp;&nbsp;&nbsp;&nbsp;author={Florence, Pete and Lynch, Corey and Zeng, Andy and Ramirez, Oscar and Wahid, Ayzaan and Downs, Laura and Wong, Adrian and Lee, Johnny and Mordatch, Igor and Tompson, Jonathan},<br>
&nbsp;&nbsp;&nbsp;&nbsp;journal={Conference on Robot Learning (CoRL)},<br>
&nbsp;&nbsp;&nbsp;&nbsp;year={2021}<br>
}</div>
            </div>
            <br>
            <div class="divider"></div>
            <br>
            


            <div class="section team">
                <h1>Team</h1>
                <div class="people-profile">
                    <a href="http://www.peteflorence.com/"><img src="images/people/pete.jpg"><p>Pete Florence</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://coreylynch.github.io/"><img src="images/people/corey.jpg"><p>Corey Lynch</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://andyzeng.github.io/"><img src="images/people/andy.jpg"><p>Andy Zeng</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://www.linkedin.com/in/oscar-ramirez-905913b9"><img src="images/people/oscar.jpg"><p>Oscar Ramirez</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://ayzaan.com/"><img src="images/people/ayzaan.jpg"><p>Ayzaan Wahid</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://www.linkedin.com/in/laura-downs-4935081"><img src="images/people/laura.jpg"><p>Laura Downs</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://almostsquare.com/"><img src="images/people/adrian.png"><p>Adrian Wong</p></a>
                </div>
                <div class="people-profile">
                    <a href="http://johnnylee.net/"><img src="images/people/johnny.png"><p>Johnny Lee</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://twitter.com/imordatch"><img src="images/people/igor.jpg"><p>Igor Mordatch</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://jonathantompson.github.io/"><img src="images/people/jonathan_tompson.jpeg"><p>Jonathan Tompson</p></a>
                </div>
                
                <div style="clear: both;"></div>
            </div>
            <div class="section teamlogo">
                <img src="images/logo.png">
                <p>Robotics at Google</p>
            </div>
            <div style="clear: both;"></div>
            <div class="divider"></div>
            
<!--             <div class="section highlights">
                <h1>Highlights</h1>
                <a href="https://ai.googleblog.com/2021/02/rearranging-visual-world.html"><img style="height: 80px; margin-left: 30px; margin-bottom: 10px" src="images/aiblog.png"></a>
                <a href="https://venturebeat.com/2020/10/28/googles-transporter-networks-learn-to-stack-blocks-and-assemble-mouthwash-kits-from-as-few-examples/"><img style="height: 26px; margin-top: 31px; margin-left: 30px; margin-bottom: 10px" src="images/vb.png"></a>
            </div>
            <div style="clear: both;"></div>
            <div class="divider"></div> -->
            
            
            
            <div class="divider"></div>
            
            <div class="section acknowledgements">
                <h1>Acknowledgements</h1>
                <p>Special thanks to Vikas Sindhwani for project direction advice; Steve Xu, Robert Baruch, Arnab Bose for robot software infrastructure; Jake Varley, Alexa Greenberg for ML infrastructure; and Kamyar Ghasemipour, Jon Barron, Eric Jang, Stephen Tu, Sumeet Singh, Jean-Jacques Slotine, Anirudha Majumdar, Vincent Vanhoucke for helpful feedback and discussions.</p><br><br>
            </div>
            <br><br><br><br><br><br><br><br><br><br>
            <div class="divider"></div>
        </div>
    </body>
</html>
